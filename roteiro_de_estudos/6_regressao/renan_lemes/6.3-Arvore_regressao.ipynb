{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Árvore de regressão.\r\n",
    "## ID3 - Interactive Dichotomiser 3.\r\n",
    "Esse algoritmo é baseado na ideia de entropia. A entropia de um dataset é uma métrica da sua incerteza ou impureza, ou seja, representa a aleatoriedade em seus valores. O seu valor é zero quando não há aleatoriedade (todos os elementos do dataset tem a mesma classificação) e aumenta conforme o dataset fica mais impuro.\r\n",
    "$\\newline$ Para deixar isso um pouco mais claro, vamos analisar o exemplo abaixo, em que o objetivo é prever se o objeto é um pentágono ou uma estrela a partir de sua cor e transparência. Inicialmente, o dataset tem entropia alta. Para diminuir essa entropia, temos dois cortes possíveis (um para cada feature). Se dividirmos pela cor da figura (a), conseguimos dois subconjuntos com entropia baixa, pois cada um tem objetos de um único tipo. Logo, o ganho de informação é alto. Já separando por transparência (b), os subconjuntos ainda são “impuros”, ou seja, têm objetos de formas diferentes. Desse modo, o ganho de informação é baixo. Por esse motivo, o algoritmo escolheria o corte da esquerda.\r\n",
    "\r\n",
    "![img](https://miro.medium.com/max/2000/1*OmFgPYbtMsOQdK6rOiRVEw.png)\r\n",
    "\r\n",
    "## CART - Classification and Regression Tree.\r\n",
    "Esse algoritmo é similar ao ID3, porém utiliza probabilidade para medir a impureza de um datase, ao invés da entropia. Imagine que você selecione duas observações aleátórias de um dataset, qual a probabilidade delas terem classes diferentes? Para que o nosso dataset seja puro, queremos que a probabilidade seja 0, ou seja, todas as observações têm a mesma classem por outro lado, conforme a impureza aumentam, esperamos que a probabilidade dos elementos terem a classes diferentes aumente.\r\n",
    "$\\newline$ Esse algoritmo pode ser utilizado para predizer valores numéricos, basicamente, nesse caso, a divisão consiste em encontrar grupos com resultados similares, e a média dos seus resultados é usada como predição para esse grupo.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pruning.\r\n",
    "Pruning consiste em podar uma árvore de decisão já treinada, em uma tentativa de diminuir o número de nós e, portanto, o overfitting. A forma mais simples de fazer isso é pelo método do erro reduzido. Esse método passa por cada nó da árvore e:\r\n",
    "* Transforma o nó em uma folha cuja classe é a mais comum no nó (dentre as observações da base de treinamento que se encaixam no nó). A sub-árvore abaixo desse nó é removida.\r\n",
    "* Verifica a acurácia da nova árvore em um conjunto de validação.\r\n",
    "* Se a acurácia do modelo não tiver diminuído, a nova árvore é mantida. Caso contrário, voltamos para a árvore antiga."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pros.\r\n",
    "* Facil explicabilidade e interpretação.\r\n",
    "* Requerem pouco esforço na preparação dos dados, métodos baseados em árvore normalmente não requerem normalização de dados.\r\n",
    "* Complexidade logarítmica na etapa de predição.\r\n",
    "* São capazes de lidar com problemas com múltiplos rótulos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contra.\r\n",
    "* Árvore crescida até sua profundidade máxima pode decorar o conjunto de treino, o que degrada seu poder preditivo quando aplicado a novos dados.\r\n",
    "* São modelos instáveis, pequena variações nos dados de treino podem resultar em árvores completamente distintas. Isso podee ser evitado ao treinarmos várias árvores.\r\n",
    "* Como visto, o algoritmo de construção da árvore de decisão é guloso, ou seja, não garante a construção da melhor estrutura para os dados de treino em questão."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Diferença entre árvore de regressão e árvore de classificação.\r\n",
    "Nós de termino (folhas) estão na parte inferior da árvore de decisão, isso significa que as árvores de decisão são desenhadas de cabeça para baixo. Dessa forma, as folhas são o fundo e as raizes são os topos, ambas as árvores trabalha de forma quase semelhantes esntre si, vamos olhar para as diferenças primárias e para as semelhanças entre as árvores:\r\n",
    "* As de regressão são usadas quando a variável dependente é continua, já as árvores de classificação são usadas quando a variável dependente é categórica.\r\n",
    "* No caso da árvore de regressão, o valor obtido pelos nós de término nos dados de treinamento é o valor médio das suas observações. Assim, a uma nova observação de dados atribui-se o valor médio correspondente.\r\n",
    "* No caso da árvore de classificação, o valor (classe) obtido pelo nó de término nos dados de treinamento é a moda das sua observações. Assim, a uma nova observação de dados atribui-se o valor da moda correspondente.\r\n",
    "* Ambas as árvores dividem o espaço preditor(variáveis independentes) em regiões distintas e não sobrepostas. Por uma questão de simplicidade, você pode pensar nessa regiões como caixas de alta dimensão ou simplesmente caixas.\r\n",
    "* Ambas as árvore seguem uma abordagem \"top-down\" e \"gananciosa\" conhecida como divisão binária recursiva \"recursive binary splitting\". É chamado de \"top-down\" por que começa do topo da árvore quando todas as observações estão disponiveis em uma única região e, sucessivamente, divide o espaço preditor em dois novos ramos no sentido inferior da árvore.\r\n",
    "* Esse processo de divisão é continuando até que um critério e parada seja definido pelo usuário seja alcançado.\r\n",
    "* Em ambos os casos, o processo de divisão resulta em árvore totalmente crescidas até que o critérios de parada sejam atigidos, mas a árvore totalmente crecida é susceptível de sobrecarga de dados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prática."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "\r\n",
    "from sklearn import tree \r\n",
    "from sklearn.datasets import load_boston\r\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "boston = load_boston()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "X = boston.data\r\n",
    "y = boston.target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "reg = tree.DecisionTreeRegressor()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "boston_tree = reg.fit(X[:-50], y[:-50])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "boston_tree.predict(X[-50:])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([13.1, 15.1, 13. , 13. , 14.1, 17.4, 21.7, 22.7, 21.7, 20.8, 13.1,\n",
       "       13.1, 10.2, 10.9, 14.1, 22.7, 19.9, 41.3, 15.1, 17.2, 14.1, 17.2,\n",
       "       14.1, 21.7, 22.7, 22.8, 41.3, 19.3, 24.7, 21.2, 17. , 22.6, 16.2,\n",
       "       15.7, 16.2, 17.4, 19.6, 16.1, 24.7, 19.8, 19.8, 18.5, 19.6, 19.8,\n",
       "       19.6, 28.4, 22.2, 23.6, 26.6, 22.2])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "boston_tree.score(X[-50:], y[-50:])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.2987398544232325"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "allScores = cross_val_score(reg, X, y, cv=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "allScores.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.10572468897291239"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
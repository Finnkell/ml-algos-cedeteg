{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe481d9",
   "metadata": {},
   "source": [
    "# 4 - Data Prep\n",
    "\n",
    "### 4.1 - Tratamento de missings\n",
    "\n",
    "&emsp; O problema de missings é prevalente na maioria das áreas de pesquisa. Missing data poduiz inúmeros problemas:\n",
    "\n",
    "* 1 - reduz o poder dos modelos estatísticos;\n",
    "* 2 - pode causar viés no modelo;\n",
    "* 3 - muitos pacotes de modelos de machine learning em python não aceitam missing data. É necessário que os dados sejam tratados primeiramente.\n",
    "\n",
    "#### 4.1.1 - Missing data mechanisms\n",
    "\n",
    "* **Missing Completely Random (MCAR)**: se os dados são completamente não relacionados para tanto observados quanto para missing instances.\n",
    "\n",
    "* **Missing at Random (MAR)**: é quando o missing data é relacionada ao dado observado, mas não a missing data.\n",
    "\n",
    "* **Missing Not At Random (MNAR)**: são os dados que não são MCAR e MAR. Isso implica que o missing data é relacionado tanto para os observados quanto para missing instances.\n",
    "\n",
    "#### 4.1.2 - Handling Missing Data\n",
    "\n",
    "* 1 - Droping Variables;\n",
    "* 2 - Partial Deletion;\n",
    "* 3 - Data Imputation;\n",
    "\n",
    "### 4.2 - Droping Variables\n",
    "\n",
    "* Exclui a coluna se possui mais de **70% de missing values** caso contrário, Data Imputation é o método mais aplicável do que simplesmente deletar. Quanto maior a informação para o modelo, maior é a confiabilidade dos resultados do modelo. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Returns dataframe containing the total missing values and percentage of total\n",
    "    missing values of a column.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df = miss_df.append(dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val])),ignore_index=True)\n",
    "    return miss_df\n",
    "\n",
    "drop_cols = df[df['PercentMissing'] >70.0].ColumnName.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e1eda",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 - Exclusão Parcial\n",
    "\n",
    "#### 4.3.1 - Listwise Deletion:\n",
    "* É uma técnica na qual as linhas que contém missing data são deletadas.\n",
    "\n",
    "* Desvantagens: diminui a intensidade dos testes estatísticos realizados. Mais amostras de dados são o fator-chave na análise. Como a listwise deletion elimina os dados com valores ausentes, diminui o tamanho dos dados. A listwise deletion pode causar distorções nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba391500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listwise_deletion(train):\n",
    "    for col in train.columns:\n",
    "        miss_ind = train[col][train[col].isnull()].index\n",
    "        train = train.drop(miss_ind, axis = 0)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ede1ce",
   "metadata": {},
   "source": [
    "### 4.4 - Data Imputation\n",
    "\n",
    "#### 4.4.1 - Single Imputation\n",
    "\n",
    "* A Single Imputation tenta imputar os dados ausentes por um único valor, em oposição à Multiple Imputation, que substitui os dados ausentes por vários valores.\n",
    "\n",
    "* **Mean Imputation**: é o processo de imputar o missing data pela média da variável e pode ser feito apenas para colunas numéricas.\n",
    "\n",
    "* **Desvantagens**: mean imputation é mais provável introduzir viés no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3395cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_imputation(train_numeric):\n",
    "    \"\"\"\n",
    "    Mean Imputation\n",
    "    \"\"\"\n",
    "    for col in train_numeric.columns:\n",
    "        mean = train_numeric[col].mean()\n",
    "        train_numeric[col] = train_numeric[col].fillna(mean)\n",
    "    return train_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15dc02f",
   "metadata": {},
   "source": [
    "#### 4.4.2 - Regression Imputation\n",
    "* Um modelo de regressão é ajustado onde os previsores são os recursos sem missing values e os destinos são os recursos com missing values.\n",
    "\n",
    "* Os missing values são então substituídos com as previsões. Regression Imputation tem uma chance menor que introduzir viés ao modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208ce02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_index(train_numeric_regr, target_cols):\n",
    "    \"\"\"\n",
    "    Returns the index of the missing values in the columns.\n",
    "    \"\"\"\n",
    "    miss_index_dict = {}\n",
    "    for tcol in target_cols:\n",
    "        index = train_numeric_regr[tcol][train_numeric_regr[tcol].isnull()].index\n",
    "        miss_index_dict[tcol] = index\n",
    "    return miss_index_dict\n",
    "\n",
    "def regression_imputation(train_numeric_regr, target_cols, miss_index_dict):\n",
    "    \"\"\"\n",
    "    Fits XGBoost Regressor and replaces the missing values with\n",
    "    the prediction.\n",
    "    \"\"\"\n",
    "    for tcol in target_cols:\n",
    "        y = train_numeric_regr[tcol]\n",
    "        '''Initially impute the column with mean'''\n",
    "        y = y.fillna(y.mean())\n",
    "        xgb = xgboost.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "        '''Fit the model where y is the target column which is to be imputed'''\n",
    "        xgb.fit(predictors, y)\n",
    "        predictions = pd.Series(xgb.predict(predictors),index= y.index)    \n",
    "        index = miss_index_dict[tcol]\n",
    "        '''Replace the missing values with the predictions'''\n",
    "        train_numeric_regr[tcol].loc[index] = predictions.loc[index]\n",
    "    return train_numeric_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b612f5f",
   "metadata": {},
   "source": [
    "#### 4.4.3 - Mode Imputation\n",
    "* É o processo de imputar missing data pela moda da variável e pode ser feito somente colunas categoricas.\n",
    "\n",
    "* **Desvantagem**: chance maior de introduzir viés ao modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c86dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_imputation(train_categoric):\n",
    "    \"\"\"\n",
    "    Mode Imputation\n",
    "    \"\"\"\n",
    "    for col in train_categoric.columns:\n",
    "        mode = train_categoric[col].mode().iloc[0]\n",
    "        train_categoric[col] = train_categoric[col].fillna(mode)\n",
    "    return train_categoric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6a08a",
   "metadata": {},
   "source": [
    "#### 4.4.4 - Multiple Imputation\n",
    "* A missing data pe imputada com multiploes valores inúmeras vezes em uma multitude de conjunto de dados imputados.\n",
    "\n",
    "\n",
    "* **MICE (Multiple Imputation by Chained Equation)**:\n",
    "    * Passo 01: Inicialmente, o conjunto de dados é imputado com a média agindo como um 'placeholder';\n",
    "    * Passo 02: Uma missing value é escolhida aleatóriamente e entitulada como o target. Um modelo de regressão é treinado nessas features e no target;\n",
    "    * Passo 03: As missings values do target são então subtituídas com previsões (imputações) do modelo de regressão;\n",
    "    * Passo 04: Os passos 02 e 03 são então repetidos para cada feature que tenha dados não observados. Ao fim de um ciclo, todos os missing values terão sido imputados com as previsões;\n",
    "    * Passo 05: Os passos 02 e 03 são então repetidos por um número específico de ciclos;\n",
    "    \n",
    "    \n",
    "* **Algoritmo MICE para dados categóricos**: antes de usarmos os passos acima precisamos seguir os passos abaixo em ordem de imputar dados categóricos:\n",
    "\n",
    "    * Passo 01: Encode Ordinal de valores non-null;\n",
    "    * Passo 02: Usar MICE com Gradient Boosting Classifier para imputar dados do Encode Ordinal;\n",
    "    * Passo 03: Converter os valores de ordinais de volta para valores categoricos;\n",
    "    * Passo 04: Seguir os passos 01 ao 05 do MICE. Ao invés de usar Mean Imputation como estratégia inicial usaremos Mode Imputation;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ef34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mice_imputation_numeric(train_numeric):\n",
    "    \"\"\"\n",
    "    Impute numeric data using MICE imputation with Gradient Boosting Regressor.\n",
    "    (we can use any other regressors to impute the data)\n",
    "    \"\"\"\n",
    "    iter_imp_numeric = IterativeImputer(GradientBoostingRegressor())\n",
    "    imputed_train = iter_imp_numeric.fit_transform(train_numeric)\n",
    "    train_numeric_imp = pd.DataFrame(imputed_train, columns = train_numeric.columns, index= train_numeric.index)\n",
    "    return train_numeric_imp\n",
    "\n",
    "def mice_imputation_categoric(train_categoric):\n",
    "    \"\"\"\n",
    "    Impute categoric data using MICE imputation with Gradient Boosting Classifier.\n",
    "    Steps:\n",
    "    1. Ordinal Encode the non-null values\n",
    "    2. Use MICE imputation with Gradient Boosting Classifier to impute the ordinal encoded data\n",
    "    (we can use any other classifier to impute the data)\n",
    "    3. Inverse transform the ordinal encoded data.\n",
    "    \"\"\"\n",
    "    ordinal_dict={}\n",
    "    for col in train_categoric:\n",
    "        '''Ordinal encode train data'''\n",
    "        ordinal_dict[col] = OrdinalEncoder()\n",
    "        nn_vals = np.array(train_categoric[col][train_categoric[col].notnull()]).reshape(-1,1)\n",
    "        nn_vals_arr = np.array(ordinal_dict[col].fit_transform(nn_vals)).reshape(-1,)\n",
    "        train_categoric[col].loc[train_categoric[col].notnull()] = nn_vals_arr\n",
    "\n",
    "    '''Impute the data using MICE with Gradient Boosting Classifier'''\n",
    "    iter_imp_categoric = IterativeImputer(GradientBoostingClassifier(), max_iter =5, initial_strategy='most_frequent')\n",
    "    imputed_train = iter_imp_categoric.fit_transform(train_categoric)\n",
    "    train_categoric_imp = pd.DataFrame(imputed_train, columns =train_categoric.columns,index = train_categoric.index).astype(int)\n",
    "    \n",
    "    '''Inverse Transform'''\n",
    "    for col in train_categoric_imp.columns:\n",
    "        oe = ordinal_dict[col]\n",
    "        train_arr= np.array(train_categoric_imp[col]).reshape(-1,1)\n",
    "        train_categoric_imp[col] = oe.inverse_transform(train_arr)\n",
    "        \n",
    "    return train_categoric_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc380eb",
   "metadata": {},
   "source": [
    "## 4.2 - Tratamento de Outliers\n",
    "\n",
    "Os outliers se referem ao pontos que estão muito distantes de onde os demais estão contidos. Ou seja, outliers são pontos raros ou distintos. Podemos usar 3 maneiras de trabalhar com eles, essas sendo:\n",
    "\n",
    "### 4.2.1 - Univariated Method\n",
    "* Um dos métodos mais simples para detecção de outliers, usando box plots. Um box plot pode ser descrito como uma representação gráfica da distribuição dos dados. Usando mediana e os interquartis. A distância máxima do centro dos dados que vai ser permitido é chamado de **cleaning parameter**. Se o **cleaning parameter** é extenso, o teste se torna menos sensitivo a outliers. Caso seja muito pequeno, muitos valores são detectados com outliers.\n",
    "\n",
    "### 4.2.2 - Multivariated Method\n",
    "* Ouliers não precisam ser valores extremos. Na verdade, as vezes o método anterior não funciona muito bem. O Multivariated Method tenta resolver criando um modelo preditivo usando todas os dados deisponíveis e limpando as instâncias com erros acima do valor dado. \n",
    "\n",
    "### 4.2.3 - Minkowski Error\n",
    "* Diferente dos métodos anteriores o Minkowski error não detecta ou limpa os outliers, ao invés ele reduz o impacto que eles terão no modelo.\n",
    "* O Minkowski Error é um Loss Index que é mais sensível aos outliers que para o MSE, que levanta cada erro da instância ao quadrado ocasionando os outliers a terem uma grande contribuição, No método Minkwski Error temos a solução onde ao invés de elevarmos ao quadrado elevamos a um número menor que 2. Esse número é chamado de parâmetro Minkowski, e reduz a contribuição dos outliers para o erro total, dando a seguinte equação: $\\text{minkowski_error} = \\frac{\\sum(\\text{outputs - targets})^{minkowski_parameter}}{\\text{instances_number}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0045f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f3c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27c60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

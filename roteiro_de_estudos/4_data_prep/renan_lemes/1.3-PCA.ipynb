{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PCA.\r\n",
    "A análise de componentes principais pode ser dividida em cinco etapas, vai ser passado cada etapa, fornecendo explicações lógicas \r\n",
    "sobre o PCA está fazendo e simpleficando o conceito matemáticos, como padronização, covariância, vetores própios e valores própios.\r\n",
    "* Padronizar a gama de variáveis iniciais contínuas.\r\n",
    "* Calcular a matriz de covariância para identificar correlações.\r\n",
    "* Calcular os autovetores e autovalores da matriz de covariância para indentificar os componentes principais.\r\n",
    "* Ciar um vetor de recursos para decidir quais componentes principais manter.\r\n",
    "* Reformar os dados ao longo dos eixos dos componentes principais.\r\n",
    "\r\n",
    "## Padronização.\r\n",
    "Matemáticamente, isso pode ser feito subtraindo a média e dividindo pelo desvio padrão para cada valor de cada variável.\r\n",
    "$$z = \\frac{value-mean}{standarddeviation}$$\r\n",
    "Desta forma todas as variáveis serão transformadas na mesma escala."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cálculo da matriz de covariâcia.\r\n",
    "O objetivo desta etapa é entender como as variáveis ​​do conjunto de dados de entrada estão variando da média entre si, ou seja, verificar se existe alguma relação entre elas, porque às vezes as variáveis ​​são altamente correlacionadas de forma que contêm informações redundantes.\r\n",
    "\r\n",
    "![img](https://cdn.builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/Principal%2520Component%2520Analysis%2520Covariance%2520Matrix.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como a covariância de uma variável  consigo mesma é sua variância, e como a covariância é comutativa então temos a matriz triangular superior e a inferior serão iguais desse modo podemos notar mais possiveis erros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O que as covariâncias que temos como entradas da matriz nos dizem sobre as correlações entre as variáveis?\r\n",
    "Na verdade, é o sinal da covariância que importa.\r\n",
    "* Se possitivo, então: as duas variáveis aumentam ou diminuem juntas(correlacionadas).\r\n",
    "* Se negativo, então: um aumenta quando o outro diminui(inversamente correlacionado).\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calcular os vetores própios e os valores próprios da matriz de covariância para indentificar os componentes principais.\r\n",
    "Autovetores e autovalores são os conceitos de álgebra linear que precisamos calcular a partir da matriz de covariância para determinar os componentes principais dos dados. Antes de chegar à explicar desses conceitos.\r\n",
    "$\\newline$ Os componentes principais são novas variáveis contruidas como combinação lineares ou misturadas das variáveis iniciais. Essas combinações são feitas de forma que as novas variáveis não estejam correlacionadas e a maioria das informalçoes dentro das variáveis iniciais seja comprimida nos primeiros componentes, ou seja, a ideia que dados 10 dimensionas forneçam 10 componentes principais, mas o PCA tenta colocar o máximo de informações possiveis no primeiro componente, e depois o máximo de informações restantes no segundo e assim por diante."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vetor de recurso.\r\n",
    "Como vimos na etapa anterior, computar os autovetores e ordená-los por seus autovalores em ordem decrescente nos permite encontrar os componentes principais em ordem de significância. Nesta etapa, o que fazemos é escolher entre manter todos esses componentes ou descartar aqueles de menor significância (de autovalores baixos), e formar com os restantes uma matriz de vetores que chamamos de  vetor de característica .\r\n",
    "\r\n",
    "Portanto, o vetor de características é simplesmente uma matriz que tem como colunas os autovetores dos componentes que decidimos manter. Isso o torna o primeiro passo para a redução da dimensionalidade, porque se escolhermos manter apenas  p  autovetores (componentes) de  n , o conjunto de dados final terá apenas  p  dimensões."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reformule os dados ao longo dos eixos dos componentes principais.\r\n",
    "Nesta etapa, que é a última, o objetivo é usar o vetor de características formado a partir dos autovetores da matriz de covariância, para reorientar os dados dos eixos originais para aqueles representados pelos componentes principais (daí o nome Análise de Componentes Principais ) Isso pode ser feito multiplicando a transposição do conjunto de dados original pela transposição do vetor de características.\r\n",
    "$$finalDataSet = FeatureVector^T \\cdot StandardizedOriginalDataSet$$"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
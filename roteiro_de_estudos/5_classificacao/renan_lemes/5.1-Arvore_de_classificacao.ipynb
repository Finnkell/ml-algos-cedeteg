{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Árvore de classificação.\r\n",
    "Classificação é um processo de duas etapas, etapa de aprendizagem e etapa de previsão. Na etapa de aprendizagem, o modelo é desenvolvido com base nos dados de treinamento fornecido. Na etapa de previsão,  o modelo é usado para prever a resposta para dados fornecidos. A árvore de decisão é um dos algoritmos de classificação mais fáceis e populares de entender e interpretar ele pode ser utilizado para o tipo de problemas de classificação e regressão.\r\n",
    "* Medidas de seleção de atributos.\r\n",
    "    * Ganho de informações.\r\n",
    "    * Razão de ganho.\r\n",
    "    * Indice de Gini.\r\n",
    "\r\n",
    "![img](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1545934190/1_r5ikdb.png)     "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Como funciona.\r\n",
    "A ideia por trás do algoritmo é a seguinte:\r\n",
    "* Selecione o melhor atributo usando Atribute Selection Measures (ASM) para dividir os registros.\r\n",
    "* Transforme esse atributo em um nó de decisão e divide o conjunto de dados em subconjuntos menores.\r\n",
    "* Inicia a construção da árvore repentindo este processo recursivo para cada filho até que uma das condições corresponda:\r\n",
    "    * Todas as tuplas pertencem ao mesmo valor atribuido.\r\n",
    "    * Não há mais atributos.\r\n",
    "    * Não há mais instâncias.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Medidas de seleção. \r\n",
    "É uma heuristica para selecionar o critério de divisão que particiona os dados de melhor maneira possível. Também é conhecido como regras de divisão por que nos ajuda a determinar pontos de interrupção para tuplas em um determinado nó. ASM fornecido uma classificação para cada recurso. O melhor atributo de pontuação será selecionado como atributo de divisão (Fonte)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ganho de informação.\r\n",
    "$$info(D) = - \\sum^{m}_{i=1} pi \\log_2 pi$$\r\n",
    "$$info_A(D) = \\sum^{V}{j=1}\\frac{|D_j|}{|D|} * X * info(D_j)$$\r\n",
    "$$Gain(A) = info(D) - info_A(D)$$\r\n",
    "\r\n",
    "* info(D) - quantidade média de informações necessária para indentificar o rótulo de classe de um tupla em D.\r\n",
    "* $\\frac{|D_j|}{D} $ Atua como o peso da j-ésima partição.\r\n",
    "* Info(D) é a informação esperada necessária para classificar uma tupla de D com base no parcionamento por A.\r\n",
    "$\\newline$ conceito de entropia,  que mede a impureza do conjunto de entrada. Em física e em matemática,  a entropia é referida como aleatoriedade ou impureza no sistema. Na teoria da informação, refere-se à impureza em grupos de exemplos, o ganho de informação é a diminuição da entropia. O ganho de informação calcula a diferença entre a entropia antes da divisão e a entropia média após a divisão do conjunto de dados com base em determinados valores atribuidos.\r\n",
    " \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prática."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
    "from sklearn import metrics\r\n",
    "from datetime import datetime as dt\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.datasets import load_iris"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "iris  = load_iris()\r\n",
    "#iris"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "X = iris.data\r\n",
    "y = iris.target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\r\n",
    "irisTree = clf.fit(X, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "irisTree.predict([[2., 2., 2., 2.]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "allScores = cross_val_score(clf, X, y , cv=10)\r\n",
    "allScores"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.        , 0.93333333, 1.        , 0.93333333, 0.93333333,\n",
       "       0.86666667, 0.93333333, 0.93333333, 1.        , 1.        ])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f763c5ae1380102371fc37b7f9e900cc55027b2a662d6fbcccf1534945890ce5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}